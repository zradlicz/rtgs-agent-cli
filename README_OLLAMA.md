# Gemini CLI with Ollama Support

This is a fork of the official [Google Gemini CLI](https://github.com/google-gemini/gemini-cli) that adds support for local LLM models through [Ollama](https://ollama.ai/). No internet connection or API keys required!

## ğŸ¦™ What's Different

- **Local Models**: Use Llama, Mistral, CodeLlama, and other open-source models locally
- **No Authentication**: No need for Google accounts, API keys, or internet connection
- **Privacy**: All processing happens on your local machine
- **Free**: No API costs or usage limits

## ğŸš€ Quick Start

### Method 1: Interactive Setup (Recommended)

1. **Install Ollama**: Download from [ollama.ai](https://ollama.ai/download)
2. **Start Ollama**: Run `ollama serve` in a terminal
3. **Pull a model**: `ollama pull llama3.2` (or your preferred model)
4. **Clone and setup**:
   ```bash
   git clone <your-fork-url>
   cd rtgs-agent-cli
   npm install
   npm run start
   ```
5. **In the authentication menu**, select "ğŸ¦™ Use Ollama (Local Models)" - it's the first option!

### Method 2: Automated Setup

1. **Prerequisites** (same as above)
2. **Clone and setup**:
   ```bash
   git clone <your-fork-url>
   cd rtgs-agent-cli
   npm install
   ```
3. **Run the setup script**:
   ```bash
   node scripts/ollama-setup.js
   ```
4. **Start the CLI**:
   ```bash
   # Option 1: Use the generated script
   ./start-ollama.sh
   
   # Option 2: Use npm script
   npm run start:ollama
   
   # Option 3: Manual setup
   source .env.ollama && npm run start
   ```

## ğŸ¯ Supported Models

The CLI works with any Ollama-compatible model. Popular choices:

### General Purpose
- `llama3.2` - Latest Llama model (recommended)
- `mistral` - Fast and capable
- `qwen2.5` - Good multilingual support

### Code-Focused
- `codellama` - Specialized for programming
- `deepseek-coder` - Excellent code understanding
- `codegemma` - Google's code model

### Lightweight
- `llama3.2:1b` - Fast, smaller model
- `phi3` - Microsoft's compact model

To change models:
```bash
# Pull the model
ollama pull mistral

# Update your .env.ollama file
GEMINI_MODEL=mistral

# Or set environment variable
export GEMINI_MODEL=mistral
```

## âš™ï¸ Configuration

### Environment Variables

Create a `.env.ollama` file (generated by setup script):

```bash
# Required
GEMINI_AUTH_TYPE=ollama
GEMINI_MODEL=llama3.2

# Optional
OLLAMA_HOST=http://localhost:11434
GEMINI_EMBEDDING_MODEL=nomic-embed-text
GEMINI_USAGE_STATISTICS_ENABLED=false
```

### Model Selection

Choose based on your needs:
- **Speed**: `llama3.2:1b` or `phi3`
- **Quality**: `llama3.1:70b` (requires more memory)
- **Code**: `codellama` or `deepseek-coder`
- **Balanced**: `llama3.2` (default)

## ğŸ› ï¸ Advanced Usage

### Custom Ollama Host

If running Ollama on a different host/port:

```bash
export OLLAMA_HOST=http://your-server:11434
```

### Multiple Models

Switch models easily:
```bash
# In the CLI
> Switch to codellama for this conversation
```

### Memory Requirements

Model memory usage (approximate):
- 1B models: ~1GB RAM
- 7B models: ~4GB RAM
- 13B models: ~8GB RAM
- 70B models: ~40GB RAM

## ğŸ”§ Development

### Building

```bash
npm run build
```

### Testing

```bash
# Test Ollama connection
curl http://localhost:11434/api/tags

# Test model
ollama run llama3.2 "Hello, how are you?"
```

## ğŸ†˜ Troubleshooting

### "Ollama not running"
```bash
# Start Ollama
ollama serve
```

### "Model not found"
```bash
# List available models
ollama list

# Pull missing model
ollama pull llama3.2
```

### "Connection refused"
```bash
# Check Ollama is running on correct port
ps aux | grep ollama

# Check environment
echo $OLLAMA_HOST
```

### Memory Issues
```bash
# Use smaller model
ollama pull llama3.2:1b
export GEMINI_MODEL=llama3.2:1b
```

## ğŸ¤ Contributing

This fork maintains compatibility with the original Gemini CLI while adding Ollama support. 

### Key Changes Made

1. **Core Integration**: Added `OllamaContentGenerator` class that implements the same interface as Gemini
2. **Authentication**: Extended `AuthType` enum with `USE_OLLAMA` and modified validation
3. **User Interface**: Added Ollama as the first option in the authentication menu with ğŸ¦™ emoji
4. **Smart Detection**: Automatically detects Ollama environment and suggests it to users
5. **Validation**: Checks if Ollama is running and has models before allowing selection
6. **Configuration**: Added Ollama-specific model configurations and setup automation

### File Structure

```
packages/core/src/
â”œâ”€â”€ ollama/
â”‚   â””â”€â”€ ollamaContentGenerator.ts    # Main Ollama integration
â”œâ”€â”€ core/
â”‚   â””â”€â”€ contentGenerator.ts         # Extended with Ollama support
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ models.ts                   # Added Ollama models
â”‚   â””â”€â”€ ollamaModels.ts            # Ollama-specific configs
â””â”€â”€ ...

packages/cli/src/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ auth.ts                     # Extended auth validation
â””â”€â”€ ...

scripts/
â””â”€â”€ ollama-setup.js                 # Setup automation
```

## ğŸ“„ License

This project maintains the same Apache 2.0 license as the original Gemini CLI.

---

**Original Project**: [google-gemini/gemini-cli](https://github.com/google-gemini/gemini-cli)  
**Ollama**: [ollama/ollama](https://github.com/ollama/ollama)

Enjoy using Gemini CLI with your local models! ğŸ¦™